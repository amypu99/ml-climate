Corporate greenwashing — where companies overstate or misrepresent their environmental efforts — poses a significant barrier to genuine climate progress. Traditional assessments, such as the Corporate Climate Responsibility Monitor (CCRM), require extensive manual labor from domain experts, limiting scalability. In this work, we present a fully automated pipeline leveraging large language models (LLMs) to replicate key elements of CCRM-style reports. Our system integrates advanced OCR for text extraction, recursive document chunking, retrieval-augmented generation (RAG), and rubric-driven prompt engineering to evaluate corporate sustainability reports at scale. We evaluate three LLMs — ClimateGPT-7B, Qwen, and Mistral — across multiple prompting and retrieval strategies, measuring both question-level accuracy and numeric precision. Our results demonstrate that chunk-by-chunk querying with RAG and embedded scoring rubrics substantially outperforms naive summarization and truncation approaches, achieving up to 0.26 overall transparency accuracy for ClimateGPT despite its shorter context window.
