## Weeks of 02/10 and 02/17 - Dataset Collection, Scoping, & Understanding

Over the past two weeks, our group has been refining our project idea and conducting extensive research to identify the best datasets and methodologies for our machine learning project on corporate climate commitments. Initially, we explored multiple potential directions, including analyzing countries' climate policies to predict emissions, examining climate-related discussions on social media, and assessing the feasibility of corporate carbon neutrality pledges.

After reviewing various academic papers and datasets, we decided to focus on using Natural Language Processing (NLP) techniques to analyze corporate climate pledges and predict whether companies are likely to achieve their stated carbon neutrality goals. This decision was based on the availability of relevant datasets and our interest in studying how corporations' climate policies translate into real-world emission reductions.

A significant portion of our research involved identifying sources of corporate climate data. We explored resources such as the CDP dataset, SEC filings, company sustainability reports, and ESG risk ratings. One of the challenges we encountered was the inconsistency in corporate emissions reporting, as different companies use different definitions for reporting periods and emissions categories (Scope 1, 2, and 3 emissions). To address this, we discussed the possibility of using ESG scores as a more standardized measure but recognized potential limitations in their correlation with actual emissions.

We also considered using ClimateBERT for feature extraction and text embeddings, as well as scraping corporate announcements and reports for policy-related information. Additionally, we explored different ways to establish ground truth labels for our model, debating whether to rely on reported emissions data, third-party ESG scores, or a combination of both.

Another key discussion point was determining our sample size. While focusing solely on Fortune 500 companies could limit our dataset size, we considered expanding our dataset by incorporating other publicly available corporate climate pledges. We also investigated government databases and financial reports to cross-check emissions data.

Moving forward, our next steps include finalizing our dataset selection, beginning the process of data collection and preprocessing, and drafting an initial project proposal. We plan to email our professor with our refined project idea and seek feedback on our approach, particularly regarding our methodology for assessing corporate climate commitments.

Overall, these past two weeks have been productive, with substantial progress in defining our project scope and identifying key challenges and solutions. We are excited to continue developing our model and refining our approach as we move forward in the project.

## Weeks of 2/24 and 3/3 - Dataset collection

Following the feedback received from the Prof. Alp and TAs, we are using this time to do the manual webscraping for sustainability reports and carbon net-zero pledges. We currently have a [spreadsheet](https://docs.google.com/spreadsheets/d/1NFO-U_q-CcS0DVRNQp9fWf4QF2BBKuGdOIawZDAE6cA/edit?gid=0#gid=0) of about 150+ Fortune 500 companies for which we've found sustainability reports/net-zero pledges for. A large percentage of these companies are tech companies, but many of them are just on the Fortune 500 list and are also non-tech.

Some things we've noticed so far with the scraping: not all companies have a pledge, and all companies' sustainability policeis look different. Some companies have their sustainability policies under a "sustainability" page, while others have it under a "corporate responsability" page. Some have made public announcements on blogs or done publicitiy advertising for their net-zero carbon pledges, whereas other companies' statements are just a  mere paragraph long. Some companies' most recent updates on their progress is from 2024, some are from 2023, and some are from 2022. Some companies' fiscal years starts in June, whereas others start in July. Some companies, such as Wells Fargo, previously had specific net-zero carbon emissions pledges, but then dropped them. This would actually be very interesting to add as an input to the model somehow.

What this means is that while we don't seem to have issues gathering data - in fact, most companies have some sort of statement on moving towards a more "sustainable future," whatever that means for them, it's still not entirely clear how much this data will be useful in training a model (given the large diversity in their policies, statements, blog posts). In the next week, we think a good idea would be to naively train an off-the-shelf LLM to predict ESG scores just to see whether any kind of relationship exists between these text policies and the ESG scores. We'll have a better idea then of how feasible the project is given the kind of data we have. In the meantime, we have also been looking elsewhere for data and applied for some access to some data held by private companies regarding carbon emissions for different tech companies.

## Week of 3/10 - Testing feasability & shifting goals from ESG Scores to Greenwashing Detection
This week, we reevaluated our approach of using large language models (LLMs) to predict ESG scores as mentioned above. After conducting a small-scale study, we realized that LLMs are not particularly effective at generating accurate ESG scores given the limited dataset available. This prompted us to refine our focus and shift toward a more feasible and impactful application. Our attention turned to the Corporate Climate Responsibility Monitor (CCRM, https://carbonmarketwatch.org/campaigns/corporate-climate-responsibility-monitor-2024/), a resource that provides in-depth reports assessing whether companies are engaging in greenwashing. These reports, compiled through extensive research by climatologists, offer a thorough evaluation of corporate sustainability claims. However, since they require significant manual effort to produce, we saw an opportunity to automate this process by leveraging LLMs to generate similar reports and systematically determine if a company is greenwashing.

We explored existing climate-specific models to determine the best option for our needs. After reviewing literature on ClimateBERT and ClimateGPT (https://arxiv.org/pdf/2401.09646), we found that ClimateGPT (https://huggingface.co/eci-io/climategpt-70b) was the more suitable choice. Unlike ClimateBERT, which was trained on paragraph-level text, ClimateGPT was pre-trained on full documents, starting with LLaMA-2 and further fine-tuned on a domain-specific dataset comprising 4.2 billion tokens. Additionally, ClimateGPT has a larger input capacity of 4,000 tokens, making it more capable of handling complex sustainability texts.

To effectively process the sustainability report, we first converted the PDF into text while ensuring proper data cleaning. This involved removing the 20 most frequently recurring words and eliminating text found in page margins, which often contained footnotes, headers, or extraneous information. Despite these efforts, we encountered another limitation. We selected Amazon’s corporate sustainability report as our initial sample and from the spreadsheet we have collected, we found that these reports are typically very lengthy, often exceeding 150 pages. Despite having access to GCP credits, we encountered technical limitations, particularly in memory capacity, when attempting to process the full document using ClimateGPT in Colab. Additionally, ClimateGPT’s 4,000-token input capacity was still insufficient for processing the entirety of Amazon’s 165-page report. Given that a few paragraphs alone could fill the token limit, we devised a solution to split the extracted text into manageable chunks. Each chunk was then processed separately through ClimateGPT, with the model generating a detailed short summary for each segment.

Once all summaries were generated, our initial plan was to concatenate them into a comprehensive document before prompting the model with targeted questions regarding greenwashing indicators. These questions would be derived from the CCRM framework, covering key areas such as tracking and disclosure transparency, integrity in emission reduction targets, and overall corporate climate responsibility. However, after further deliberation, we realized that summarizing each chunk first could lead to information loss.

Moving forward, we plan to enhance our approach by integrating Retrieval-Augmented Generation (RAG) into the summarization process, as ClimateGPT has demonstrated strong performance when combined with RAG techniques. Additionally, we will refine our prompt engineering strategy by shifting away from general summaries and instead directly querying the model about specific aspects of greenwashing within each text chunk. This targeted approach should preserve more relevant information and produce more precise insights compared to our initial method of summarization followed by broad questioning. Our goal for this week is to implement these refinements and evaluate the effectiveness of the new strategy.

## Week of 3/24 - Compiling extra sources to increase training dataset size for running baselines
This week, we received advice from Prof. Alp that we should try to find more training data and labels for generating reports, even if the labels are worse quality than the data from CCRM. We sought out some other resources, looking into both SBTi (Science Based Targets) and Transition Pathways, both of which are climate action organizations that develop standards, tools, guidance, and reports on different companies’ greenhouse gas emissions and their targets. SBTi has their own evaluation dataset detailing whether a company has set certain targets, but the evaluation is not as detailed as the CCRM. Transition Pathways dataset includes a series of evaluation questions with answers per company regarding the greenhouse gas emissions. 

Looking at the intersection of both datasets and using the evaluations from SBTi and the questions from TransitionPathways, we were able to create an additional 150 labels. SBTi and Transition Pathways datasets each have more than 1k labeled data, but many of them are non-US companies, which would not be suitable for our task, since we rely on English reports as training data. We also scraped the sustainability reports for the 150 companies in both datasets. This brings our total dataset including both training data and labels around 200-300 examples. We can also increase our dataset even more if we decide to look into companies that exist in either dataset but not both (for example, a company is evaluated by SBTi but not Transition Pathways).

Additionally, we worked on taking the graphic reports from CCRM and turning them into questions we could prompt the model (labels we’d use for training). We looked through the graphic reports to identify which questions were most important in a summary report for a companies’ GHG emissions. In order to do this, we actually used pixel constrains in identifying different sections of the graphic report that contained questions we wanted as labels, and parsed the image into answers for our questions. The CCRM authors also used colors to indicate the assessment of transparency for certain evaluations; we used pixel constraint to select the symbol with the corresponding color and used the metadata of the color extracted as our label. For example, using the PDF reader, we extracted “Integrity: Average RGB Color: [241. 106.  33.]” which would correspond to the label“Poor”.

Some issues we ran into while creating the labels were that because we are looking at evaluations from different organizations, the evaluation from both organizations can be from different years. We had to manually check whether SBTi’s and Transition Pathways’  assessment years matched together, and whether we would have the corresponding report of the company from the previous fiscal year. There is also the issue of matching the fiscal year with the actual year of the report, which is still an ongoing issue, and could lead to issues in training later. Companies’ names also differ slightly based on the organization. For instance, Transition Pathways would use “Adobe” whereas SBTi would use “Adobe, Inc.” We used some fuzzy matching logic to join the datasets together on company name and year.

Another issue we had was that for many of the companies in the CCRM dataset, which includes better quality labels, these companies did not have labels in the SBTi or Transition Pathways dataset. We hope to use interpolation to create labels where a company is missing those evaluations.

For next steps, given our 200+ example dataset, we will run the PDF reader on all of the company reports, and run some baselines using ClimateGPT in predicting the questions we found important for generating a report. After getting baselines, we'll look into how we can improve these questions.

## Week of 3/31 - Cleaning, Deduplication, and Resolving Temporal Conflicts Across Datasets
This week, we focused on refining and reconciling inconsistencies between the SBTi, Transition Pathways, and CCRM datasets, which we had begun integrating the week prior. A key issue we encountered was the misalignment of assessment years across these datasets. For example, SBTi’s most recent assessments for some companies dated back to as early as 2017, which posed problems when aligning them with more recent evaluations from Transition Pathways or CCRM. As a result, we chose to discard any SBTi datapoints with outdated assessment years (pre-2020), as these would introduce noise and weaken the relevance of our labels.

For the remaining entries, we took a careful approach in joining data across sources. If a company had matching assessment years between SBTi and Transition Pathways, we merged these into a single row in our CSV file. In cases where the years differed, we treated them as separate data points and recorded them in distinct rows to preserve temporal integrity. We also attempted to match these rows with CCRM evaluations, but only included the CCRM data if its assessment year aligned with either the SBTi or Transition Pathways entry.

This alignment strategy allowed us to avoid false correlations but introduced a new challenge: sparsity. Due to temporal mismatches, we ended up with many rows that had incomplete features or missing labels. To address this, we used basic nearest-neighbor interpolation to impute missing values in the SBTi and Transition Pathways fields. However, for the CCRM dataset—which consists of textual summaries and evaluation labels derived from visual cues—we could not interpolate missing values in the same way. Instead, we opted to leave those fields blank, acknowledging that CCRM values are not easily imputed due to their qualitative nature.

Alongside this, we finalized our first version of questions.txt, a comprehensive list of detailed prompts we’ll use to probe LLMs for greenwashing indicators based on sustainability reports. We also completed downloading all the sustainability reports for our target set of 100 companies, which we’ll use to run baseline experiments moving forward. This marks a major step forward in having both our labeled dataset and corpora ready for modeling.

## Week of 04/07 - Baseline Experiments and Early Testing with ClimateGPT & RAG
This week, we conducted our first round of baseline experiments using ClimateGPT, now equipped with our finalized questions.txt prompt set. We tested two main prompting strategies for extracting structured answers from lengthy sustainability reports: (1) applying all questions to each individual chunk of text, and (2) summarizing all chunks first and then applying the questions to the combined summary.

From our early experiments, strategy (1)—asking questions directly on each chunk—outperformed strategy (2) by a considerable margin. When prompted on the aggregated summary of all chunks, the model tended to lose granularity and failed to retrieve specific, fact-based information (e.g., numeric emissions values or fiscal data). In contrast, the chunk-by-chunk approach yielded more precise answers, albeit with occasional redundancy. Based on this, we plan to move forward using the per-chunk strategy for prompting.

We also experimented with replacing our existing PDF-to-text pipeline with olmocr, a potentially more accurate method of extracting structured text from sustainability PDFs. Early testing suggests that olmocr handles footnotes and sidebars more gracefully than our original script, but the output still requires tuning—particularly in separating paragraphs cleanly and avoiding repeated headers.

Additionally, we integrated Retrieval-Augmented Generation (RAG) into our LLM pipeline, as suggested in recent research and ClimateGPT documentation. While RAG improved the model's ability to surface relevant sections of the report for a given question, the answers themselves were still somewhat imprecise. For instance, when asked about company revenue, the model might respond with "$45 billion" when the correct figure was "$35 billion." This suggests that while RAG helps locate the general vicinity of an answer, it does not guarantee accuracy in extraction—particularly for numeric values.

Interestingly, we observed that the model sometimes performed better without RAG when asked direct factual questions. For example, extracting emissions data or target years tended to be more accurate in the standard prompting setup. We plan to investigate whether a hybrid method might work best—using RAG to surface evidence and the base model to extract the final answer.

Overall, we’ve made solid progress on the modeling front, and our next steps will include improving our chunk splitting algorithm, refining our PDF extraction pipeline using olmocr, and fine-tuning prompts to strike a better balance between specificity and interpretability.

## Week of 04/14
This week centered on improving the prompting strategy for climate-related question answering, resolving model-specific context issues, expanding the labeled dataset, and generating model outputs at scale.

The initial focus was on testing the first five Transition Pathways (TP) questions using a retrieval-augmented prompting setup. The questions were run with k_docs = 5 and the instruction: "Answer with only one 'Yes', 'No', or 'Not Applicable'". Inference was executed using max_new_tokens = 1, which produced stable and concise answers compared to previous settings (max_new_tokens = 200/256) that caused verbose and inconsistent outputs. Across models like ClimateGPT, Qwen, and Mistral, the revised prompting format performed well, often aligning with expected labels. There were instances where the model output differed from the TP label (e.g., outputting “Yes” when the TP label was “No”), but upon manual inspection of the retrieved documents, the model’s response was justifiable based on textual evidence. This highlighted some ambiguity in the source labeling or the input documents used.

To better align with each model’s capabilities, the number of documents retrieved (k_docs) was adjusted dynamically depending on model context window size. ClimateGPT, with a shorter effective context, performed better with fewer documents, while models like Qwen were more robust with longer inputs.

A new jsonl file containing all TP questions was created to enable batch execution. Inference was then run across multiple model-question combinations, including:
1. ClimateGPT on all CCRM companies for both CCRM and TP questions
2. Qwen and Mistral on all CCRM companies for CCRM questions
3. olmOCR-based conversion of CCRM company reports into chunked JSONL for input.

## Week of 04/21
To address the limited number of labeled examples (~100 companies), label interpolation was implemented for additional TP-only companies. This involved identifying companies present only in the TP dataset (excluding CCRM) and using feature-based similarity to interpolate CCRM-style labels (e.g., transparency/integrity scores). Encoding categorical fields and applying label transfer logic took ~2 hours of implementation. Fortune 500 tech companies from the TP dataset were prioritized for interpolation, followed by additional random TP companies to reach a target of 300 labeled rows.

All interpolated companies had their sustainability reports located and processed via OCR, which took **several hours** to complete. These documents were then used to run ClimateGPT on both CCRM and TP question sets, yielding model outputs for all interpolated entries.

Issues in CSV formatting caused by LLM answers containing commas were also resolved. Previously, the output files failed to parse correctly due to comma-based misalignment between columns. The formatting logic was updated to handle such cases.

Model evaluation was then performed by comparing generated responses with interpolated and ground-truth labels. Model performance varied slightly across question types and document length. Prompt tuning (e.g., explicitly setting single-token constraints) and temperature adjustments were also explored to reduce gibberish outputs. All results were compiled into a final merged dataset, and a visual breakdown was prepared for presentation in class on Monday.

## Weeks of 04/28 and 05/05

Over the past two weeks, we focused on finalizing our evaluation pipeline, consolidating our codebase, and preparing our deliverables for submission.

On the evaluation side, we integrated updated CCRM and TPI labels into our results, refined the evaluation scripts, and completed assessments across all three models. We also added support for token counting and finalized the TPI evaluation logic. These updates enabled us to generate complete result sets for both CCRM-style rubric scoring and binary TPI questions.

On the infrastructure side, we significantly cleaned up the repository. We removed deprecated files (e.g., older RAG scripts), reorganized directories (including the creation of a centralized climate_reports folder), and standardized file paths throughout. We also added documentation detailing the location of reports, labels, and prompts, and updated the README with clearer descriptions and correct file references.

Lastly, we prepared two versions of our final paper: one formatted according to the course LaTeX template and another in a two-column layout with additional detail. Both versions were added to the repository along with a final update to the README summarizing the paper structure and evaluation results.

Overall, this sprint was focused on documentation, cleanup, and final synthesis ahead of submission.
